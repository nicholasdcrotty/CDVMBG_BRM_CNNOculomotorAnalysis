{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicholasdcrotty/CMBG_BRM_CNNOculomotorAnalysis/blob/main/analysisForUser/CMBG_BHR_CNNBasedAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI-u-xCg8M5Z"
      },
      "source": [
        "\n",
        "\n",
        "# Using CNNs to analyze oculomotor timecourse data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this code, make sure to select a fast runtime (either your own local runtime or one provided by Colab), as the model fitting procedure and SHAP analysis takes quite some time on the CPU alone.\n",
        "\n",
        "In order for the CNNs to be applied correctly, your data should be in the form of a .csv file for each feature, with trials as rows and samples as columns. Your conditions file should be in the form of a .csv file, with trials as rows. **Importantly,** the order of the trials in the feature dataframes should match that of the conditions dataframe, so that trials are correctly assigned to their coresponding labels.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LXgnnJqpu3I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before running this code**, you need to load your data files into the Colab environment. To do so, click the file icon on the left sidebar, click the \"Upload to session storage\" icon (the page icon with the upwards arrow) and upload your features and conditions files."
      ],
      "metadata": {
        "id": "caf_6p6cK_aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in necessary libraries"
      ],
      "metadata": {
        "id": "BVYiAH0HIaMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq6Gzp5X8M5Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "102e3723-12ab-4c09-b4f5-351d30da1544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.47.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.61.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.44.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Downloading shap-0.47.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.47.2 slicer-0.0.8\n"
          ]
        }
      ],
      "source": [
        "# libraries related to neural network\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# libraries related to input/output arrays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#libraries related to importing data into script\n",
        "from google.colab import files\n",
        "\n",
        "#libraries related to graphing NN results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#SHAP values\n",
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-define values of experimental details based on your data"
      ],
      "metadata": {
        "id": "L4S8-NjmrvIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desiredLabel = \"targetLocation\" #what is the column name of the values you are trying to predict?\n",
        "inputFeatures = 2 #how many features does your input data have?\n",
        "outputClasses = 6 #how many unique values could your predicted label have?\n",
        "removeNAs = False #Are there NAs in your data/labels (e.g., a distractor not appearing on some trials), and should they be removed?\n",
        "removeRowLabels = True #does your .csv file have row labels that need to be removed?\n",
        "numericallyEncode = True #Do your labels need to be numerically encoded?\n",
        "\n",
        "details = {\"desiredLabel\": desiredLabel, \"inputFeatures\" : inputFeatures, \"outputClasses\" : outputClasses, \"removeNAs\" : removeNAs, \"removeRowLabels\" : removeRowLabels, \"numericallyEncode\" : numericallyEncode}\n",
        "print(f\"CNN predicting {desiredLabel} from user's dataset, with {inputFeatures} input features and {outputClasses} output classes\")"
      ],
      "metadata": {
        "id": "LQKJ6GwlrvZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726672ff-72b1-4447-d01d-0c67a2d12455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN predicting targetLocation from user's dataset, with 2 input features and 6 output classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in data\n",
        "Make sure to add objects below if you are using more than 2 features, formatted like the following: `feature3 = pd.read.csv(\".csv\")`. Then, make sure to add those objects to the `np.stack` call when formatting into a 3D numpy array."
      ],
      "metadata": {
        "id": "dmAwA_TSQ2XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upload = files.upload() #The GUI is actually much faster at this\n",
        "\n",
        "feature1 = pd.read_csv('CMBG_BRM_Section4_XPos.csv')\n",
        "feature2 = pd.read_csv('CMBG_BRM_Section4_YPos.csv')\n",
        "#repeat above for as many features as you are uploading, or remove feature2 if using one feature\n",
        "conditions = pd.read_csv('CMBG_BRM_Section4_Conditions.csv')\n",
        "\n",
        "\n",
        "#format data into 3d numpy array\n",
        "if (details[\"inputFeatures\"] == 1):\n",
        "  arr2d = feature1.values\n",
        "  data = arr2d[:, np.newaxis,:]\n",
        "else:\n",
        "  data = np.stack((feature1, feature2), axis=1) #add more features if needed\n",
        "\n",
        "#remove row labels if needed\n",
        "if details[\"removeRowLabels\"] == True:\n",
        "  data = data[:,:,1:]\n",
        "\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "RNOrAMS-SglC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cde124-acf6-4443-bbe0-1abc72b8eb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34560, 2, 600)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract desired labels from conditions datafile\n",
        "If `numericallyEncode = True`, unique value within your labels will be assigned to its own number, ranging from 0 to `outputClasses - 1`. This numerical encoding allows for the proper comparison of the CNN's output to the label during fitting."
      ],
      "metadata": {
        "id": "A1YKRS0cv8oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = details['desiredLabel']\n",
        "\n",
        "#remove NA values if necessary (e.g., if search object is absent on some trials)\n",
        "if details[\"removeNAs\"] == True:\n",
        "  data = data[conditions[label].isna()==False, :, :]\n",
        "  conditions = conditions[conditions[label].isna()==False]\n",
        "\n",
        "#extract label information from conditions file - this will serve as the comparison during supervised learning\n",
        "#numerical encoding of labels\n",
        "if details[\"numericallyEncode\"] == True:\n",
        "  oldLabels = conditions[label].astype(str)\n",
        "  newLabels = oldLabels #preallocate for proper filling of numerically encoded labels\n",
        "  uniques = oldLabels.unique()\n",
        "  for i in range(len(uniques)):\n",
        "    newLabels = newLabels.replace(uniques[i], i)\n",
        "\n",
        "  #check that labels encoded properly\n",
        "  for i in range(len(uniques)):\n",
        "    print(uniques[i], \",\", newLabels[oldLabels==uniques[i]].unique())\n",
        "  labels = newLabels.astype(int)\n",
        "else:\n",
        "  labels = conditions[label].astype(int)"
      ],
      "metadata": {
        "id": "7H3FVtj4v3U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec56ad2-3a7d-4992-8ccd-120fa957bea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180 , [0]\n",
            "120 , [1]\n",
            "60 , [2]\n",
            "300 , [3]\n",
            "240 , [4]\n",
            "0 , [5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-3200644033>:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  newLabels = newLabels.replace(uniques[i], i)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training-validation split (66% training, 33% validation)"
      ],
      "metadata": {
        "id": "c-NfFNwuxivI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, R, C = data.shape\n",
        "\n",
        "#find number of training/test trials (if data is not divisible by 3, add extras to training set)\n",
        "n_test = N // 3 #same as floor(N/3)\n",
        "n_train = N - n_test\n",
        "\n",
        "#predefine dataframes with corresponding trial lengths\n",
        "training_data = np.zeros((n_train, R, C))\n",
        "test_data = np.zeros((n_test, R, C))\n",
        "\n",
        "training_labels = []\n",
        "test_labels = []\n",
        "\n",
        "trainCounter = 0\n",
        "testCounter = 0\n",
        "\n",
        "for i in range(0, len(data)):\n",
        "  if i % 3 == 0: #every third trial, add data and label to validation set\n",
        "    test_data[testCounter, :, : ] = data[i, :, :]\n",
        "    test_labels.append(labels.iloc[i])\n",
        "    testCounter += 1\n",
        "  else: #otherwise, add data and label to training set\n",
        "    training_data[trainCounter, :, : ] = data[i, :, :]\n",
        "    training_labels.append(labels.iloc[i])\n",
        "    trainCounter += 1\n",
        "\n",
        "training_labels = pd.Series(training_labels)\n",
        "test_labels = pd.Series(test_labels)\n",
        "\n",
        "print(training_data.shape, test_data.shape)\n"
      ],
      "metadata": {
        "id": "mmk-AIP3xqOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process data"
      ],
      "metadata": {
        "id": "m9mb9MoATOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform numpy data to tensor\n",
        "training_data = torch.from_numpy(training_data)\n",
        "test_data = torch.from_numpy(test_data)\n",
        "\n",
        "#change to float type - more easily handled by training + test loops\n",
        "training_data = training_data.float()\n",
        "test_data = test_data.float()"
      ],
      "metadata": {
        "id": "pGX7ZkgOUhC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check dimensions of test data and labels"
      ],
      "metadata": {
        "id": "edIcRkTowJDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data.shape)\n",
        "#trials, features, samples\n",
        "\n",
        "print(test_labels.shape)\n",
        "#1-dimensional tensor of identical length to rows of test data"
      ],
      "metadata": {
        "id": "BBQ8GhxzwO3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine size of flattened output passed to first linear transform -- different based on the length of your trial and number of features"
      ],
      "metadata": {
        "id": "Ph5-Bt0mEJx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizeChecker = nn.Sequential( #the same layers as the first portion of the CNN, just to see what that layer's output size will be\n",
        "        nn.Conv1d(in_channels = details[\"inputFeatures\"], out_channels = 64, kernel_size = 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.MaxPool1d(kernel_size=5),\n",
        "        nn.Flatten()\n",
        "        )\n",
        "tmp = sizeChecker(test_data[0:1,:,:])\n",
        "flattenOutput = tmp.shape[1]\n",
        "print(flattenOutput)"
      ],
      "metadata": {
        "id": "f-1TWELREcce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the neural network\n"
      ],
      "metadata": {
        "id": "yN8rLWuNkoCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Flattening layer (gets called after Sequential convolution layer)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #Sequential layer containing convolution and subsequent regularization methods\n",
        "        self.cnnL1 = nn.Sequential(\n",
        "        nn.Conv1d(in_channels = details[\"inputFeatures\"], out_channels = 64, kernel_size = 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.MaxPool1d(kernel_size=5),\n",
        "        )\n",
        "\n",
        "        #Linear transforms\n",
        "        self.lin = nn.Linear(in_features = flattenOutput, out_features = 64)\n",
        "        self.lin2 = nn.Linear(in_features = 64, out_features = 32)\n",
        "        self.lin3 = nn.Linear(in_features = 32, out_features = details[\"outputClasses\"]) #Output is X number of logits, to be used as the CNN's predictions with loss function\n",
        "\n",
        "    def forward(self, x):\n",
        "        #apply convolution\n",
        "        conv = self.cnnL1(x)\n",
        "\n",
        "        #rearrage resulting array for proper order during flattening\n",
        "        permuted = conv.permute(0,2,1)\n",
        "\n",
        "        #flatten array into one-dimensional tensor\n",
        "        flattened = self.flatten(permuted)\n",
        "\n",
        "        #linear transforms, with logits as output\n",
        "        linear = self.lin(flattened)\n",
        "        linear2 = self.lin2(linear)\n",
        "        logits = self.lin3(linear2)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "ACZkKG4W8ljE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Early Stopping class"
      ],
      "metadata": {
        "id": "VMZ14NwJn5a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EarlyStopping class used in pytorchtools library\n",
        "#source: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "h4-kQmcMn5jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Custom Dataset object that stores the samples and the labels together for each trial"
      ],
      "metadata": {
        "id": "8QBhH8cutUmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, labelsObject, dataObject, transform=None, target_transform=None):\n",
        "        self.labels = labelsObject\n",
        "        self.dataframe = dataObject\n",
        "        self.transform = transform #in case data needs to be transformed to a different type\n",
        "        self.target_transform = target_transform #in case labels need to be transformed to a different type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataframe[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "xJXrHXi_D1Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the CustomDatasets for training and validation data"
      ],
      "metadata": {
        "id": "3XSb2czvtzbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = CustomDataset(labelsObject = training_labels, dataObject = training_data)\n",
        "test = CustomDataset(labelsObject = test_labels, dataObject = test_data)"
      ],
      "metadata": {
        "id": "xDWngfZatzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoader objects containing the training and test data/labels\n",
        "After our data and labels have been stored in our Custom Dataset objects, we need to place these objects into an object called a Dataloader. Dataloaders are PyTorch objects designed to divide input files into a series of minibatches and feed them into a CNN. This batching process allows the network to update its parameters more frequently than if it was fit to all of the data at once. The size of each minibatch, controlled by ``batch_size``, is the number of trials propagated through the CNN before the parameters are updated. In the current study, we chose a minibatch size of 64 trials."
      ],
      "metadata": {
        "id": "gMMqp0N4UzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, drop_last=False)\n",
        "test_dataloader = DataLoader(test, batch_size=batch_size, drop_last=False)"
      ],
      "metadata": {
        "id": "lnJkTN0HFjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiT4f2wY8M5a"
      },
      "source": [
        "## Hyperparameters & Optimizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxegQ-898M5a"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "epochs = 2500 #arbitrary large number to give time for early stopping to occur\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJGLYx-r8M5b"
      },
      "source": [
        "\n",
        "## Full Implementation\n",
        "Here, we create our respective loops for the training/validation process with one function for training and one function for validation. The validation function also stores the trial-level accuracies of the CNN, which will be used to graph the CNN's classification accuracy later on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eshSqRXG8M5c"
      },
      "outputs": [],
      "source": [
        "def train_model(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train() #set the model to training mode - important for batch normalization and dropout layers\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        #compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print current loss every 20 batches\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_model(dataloader, model, loss_fn, trialLevelDF, epNum = 0, earlyStop=\"none\", lossChecker = \"none\", mode = \"loss\"):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval() #set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    #evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    with torch.no_grad():\n",
        "      batchCount = 0\n",
        "      for X, y in dataloader:\n",
        "          pred = model(X) #apply model to batch\n",
        "          predSize = len(pred)\n",
        "          for p in (range(predSize)): #iterate through current batch to get trial-level accuracies\n",
        "\n",
        "              #determine whether there was a correct or incorrect prediction on the current dataframe\n",
        "              trialAcc = (pred[p].argmax().item() == y[p].item())\n",
        "\n",
        "              #save trial-level accuracy to initialized dataframe\n",
        "              trialLevelDF.iloc[(batch_size*batchCount)+p,epNum] = trialAcc\n",
        "\n",
        "          #update loss and accuracy metrics for reporting\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item() #same calculation as above, but performed for entire batch at once\n",
        "          batchCount+=1\n",
        "\n",
        "    #compute overall loss and accuracy\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    #if we want to save the CNN's lowest loss value, record that minimum loss and the epoch where it occurred\n",
        "    if lossChecker != \"none\" and test_loss < lossChecker[0]:\n",
        "      lossChecker[0] = test_loss\n",
        "      lossChecker[1] = epNum\n",
        "\n",
        "    #if we want to implement early stopping, apply early stopping\n",
        "    if earlyStop != \"none\":\n",
        "      earlyStop(test_loss, model)\n",
        "\n",
        "    #print the relevant performance metrics -- loss, accuracy, or both\n",
        "    if mode == \"loss\":\n",
        "      print(f\"Epoch {epNum+1} complete! \\n Current loss: {test_loss:>8f}    Lowest loss: {lossChecker[0]:>8f} \\n\")\n",
        "    elif mode == \"accuracy\":\n",
        "      print(f\"Accuracy: {(100*correct):>8f}% \\n\")\n",
        "    elif mode ==\"both\":\n",
        "      print(f\"Epoch {epNum+1} complete! \\n Current loss: {test_loss:>8f}    Lowest loss: {lossChecker[0]:>8f} \\n Accuracy: {(100*correct):>8f}% \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mScO9OrS8M5c"
      },
      "source": [
        "# Implement training and test procedures\n",
        "\n",
        "Here, we initalize a dataframe to store the trial-level validation accuracies and implement the training and validation functions we created earlier. We also save the trial-level accuracies as a .csv file to store the NN's current performance (as the fitting process is stochastic).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz5_Cozo8M5c"
      },
      "outputs": [],
      "source": [
        "#for storing CNN validation accuracy in each epoch\n",
        "performanceSummary = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(epochs))\n",
        "\n",
        "#early stopping object\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "#for storing minimum loss value and epoch where it occurred\n",
        "lossCheck = [np.inf, 0]\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    #training\n",
        "    train_model(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    #validation\n",
        "    test_model(test_dataloader, model, loss_fn, performanceSummary,\n",
        "               epNum = t, earlyStop = early_stopping, lossChecker=lossCheck, mode = \"loss\")\n",
        "\n",
        "    #check early stopping criteria\n",
        "    if early_stopping.early_stop:\n",
        "      print(f\"Early stopping @ epoch {t+1}\")\n",
        "      break\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick summary of model results\n",
        "Once our CNN has been fit to the data, we can take a quick look at its accuracy in predicting target location from eye position using the trial level accuracies. Here, we graph the accuracy across all run epochs, and identify the accuracy that occurred when the network demonstrated minimum loss. This accuracy value is the one we use as a metric of model performance. The associated parameter weights that gave rise to this performance have been saved as `checkpoint.pth`."
      ],
      "metadata": {
        "id": "GMhTBfUHjs1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bestEpoch = lossCheck[1]\n",
        "performanceSummary = performanceSummary.iloc[:,0:t+1] #in case early stopping happens\n",
        "epAcc = (100*(performanceSummary.sum() / len(test_dataloader.dataset)))\n",
        "bestPerformance = epAcc[bestEpoch]\n",
        "\n",
        "#plot accuracy across epochs and label accuracy from interation with lowest validation loss\n",
        "plt.plot(range(t+1), epAcc)\n",
        "plt.arrow(x = bestEpoch, y = bestPerformance - 0.1*bestPerformance, dx = 0, dy = 0.1*bestPerformance, width = .2)\n",
        "plt.text(x = bestEpoch-0.33*bestEpoch, y = bestPerformance - 0.2*bestPerformance, s = f\"Accuracy from epoch {bestEpoch} (starting \\n at 0) with the lowest loss -- \\n we use this in analysis!\")\n",
        "plt.text(x = bestEpoch-0.33*bestEpoch, y = bestPerformance - 0.3*bestPerformance, s = \"Note: Overfitting may have \\n happened in later epochs, \\n causing a severe accuracy \\n decrease\")\n",
        "\n",
        "#save graph of performance across epochs\n",
        "plt.savefig(\".png\")\n",
        "files.download(\".png\")\n",
        "\n",
        "#view graph and accuracy values across epochs\n",
        "plt.show()\n",
        "print(epAcc)"
      ],
      "metadata": {
        "id": "9TFvZMZk1p5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View accuracy from epoch with lowest loss"
      ],
      "metadata": {
        "id": "0Bs1EujVMNlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load in best weights\n",
        "bestWeights = torch.load(\"checkpoint.pth\")\n",
        "model.load_state_dict(bestWeights)\n",
        "\n",
        "#check of prediction accuracy for model\n",
        "trialLevelAcc = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(1))\n",
        "test_model(test_dataloader, model, loss_fn, trialLevelAcc, mode = \"accuracy\")"
      ],
      "metadata": {
        "id": "AKMDIOaePgmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download trial level accuracy as .csv file through Google Drive"
      ],
      "metadata": {
        "id": "pv_4aFEBFmPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GKab3KGuMwOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelAcc.to_csv('.csv')\n",
        "files.download('.csv')"
      ],
      "metadata": {
        "id": "x9yZ_J6Rmcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model parameter weights locally and download to computer"
      ],
      "metadata": {
        "id": "7_Yr1zQZrWeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \".pth\") #.pth file is type used for storing parameter weights\n",
        "files.download(\".pth\")"
      ],
      "metadata": {
        "id": "dYILF6j2We5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate SHAP values for each trial of validation set\n",
        "Fair warning, this takes a substantial amount of time to run."
      ],
      "metadata": {
        "id": "JAjSseyvBIWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelSHAP = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(len(test_dataloader.dataset[0][0][0])))\n",
        "print(trialLevelSHAP.shape)\n",
        "\n",
        "shapBatch = test_dataloader.dataset[0:100] #first 100 trials used for explanation model\n",
        "explainer = shap.DeepExplainer(model, shapBatch[0])\n",
        "\n",
        "loopDur = len(test_dataloader.dataset)\n",
        "for s in range(loopDur):\n",
        "\n",
        "  #calculate SHAP values relative to explainer set\n",
        "  shap_values = explainer.shap_values(test_dataloader.dataset[s:s+1][0])\n",
        "\n",
        "  #compute global feature importance by taking absolute value\n",
        "  abs_shap_values = np.abs(shap_values)\n",
        "\n",
        "  #average across classes and features to get one global feature importance value per sample\n",
        "  SHAP = abs_shap_values.mean(axis=(0,1,3))\n",
        "\n",
        "  #save trial-level accuracy to initialized dataframe\n",
        "  trialLevelSHAP.iloc[s,:] = SHAP\n",
        "  if s % 100 ==0:\n",
        "    print(s)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "WEkHk05hDwAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download SHAP values as .csv file"
      ],
      "metadata": {
        "id": "LK2cKbmYJiji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelSHAP = pd.DataFrame(trialLevelSHAP)\n",
        "trialLevelSHAP.to_csv('.csv')\n",
        "files.download('.csv')"
      ],
      "metadata": {
        "id": "Hu5KpAP6jBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graph heatmap of SHAP values for last trial as a quick visualization"
      ],
      "metadata": {
        "id": "HslO2GRcIbmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"SHAP values for each sample in timecourse\": abs_shap_values.mean(axis=(0,1,3))},\n",
        "                  index=range(len(shap_values[0][0])))\n",
        "g = sns.heatmap(df, fmt=\"g\", cmap='viridis')\n",
        "g.set_yticks(range(0,len(test_data[1,1,:]),50))\n",
        "g.set_yticklabels(range(0,len(test_data[1,1,:]),50))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YbdUOf8I3VXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}