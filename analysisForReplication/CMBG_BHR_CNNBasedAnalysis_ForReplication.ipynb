{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicholasdcrotty/CMBG_BRM_CNNOculomotorAnalysis/blob/main/analysisForReplication/CMBG_BHR_CNNBasedAnalysis_ForReplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI-u-xCg8M5Z"
      },
      "source": [
        "\n",
        "\n",
        "# Using CNNs to analyze oculomotor timecourse data - Replicating observed results from Crotty, Massa, Benson, & Grubb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running this code, make sure to select a fast runtime (either your own local runtime or one provided by Colab), as the model fitting procedure and SHAP analysis takes quite some time on the CPU alone. **NOTE: Due to the inherent stochasticty in the neural network fitting process, the resulting classification accuracy may be slightly different from the values reported in the manuscript.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LXgnnJqpu3I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before running this code**, you need to load the data files from the corresponding studt into the Colab environment. To do so, click the file icon on the left sidebar, click the \"Upload to session storage\" icon (the page icon with the upwards arrow) and upload the matching features and conditions files. There should be two feature `.csv` files and one conditions `.csv`. file"
      ],
      "metadata": {
        "id": "2rPv2WIwLHar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in necessary libraries"
      ],
      "metadata": {
        "id": "BVYiAH0HIaMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq6Gzp5X8M5Z"
      },
      "outputs": [],
      "source": [
        "# libraries related to neural network\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# libraries related to input/output arrays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#libraries related to importing data into script\n",
        "from google.colab import files\n",
        "\n",
        "#libraries related to graphing NN results\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#SHAP values\n",
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indicate which set of results to replicate here!"
      ],
      "metadata": {
        "id": "o1exGL0b2Qnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "potentialReplications = [\"massaTarget\", \"grubbLiTarget\", \"massaDistractor\", \"grubbLiDistractor\"] #choose from one of these four\n",
        "replicateAnalysis = \"grubbLiDistractor\""
      ],
      "metadata": {
        "id": "26UuV_Y91vvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Update script with correct experimental details"
      ],
      "metadata": {
        "id": "HmtRgpaLAAQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#details for first section of results - CNN predicting target location from Massa et al. (2024) dataset\n",
        "if replicateAnalysis == \"massaTarget\":\n",
        "  details = {\"desiredLabel\": \"targetLocation\", \"inputFeatures\" : 2, \"outputClasses\" : 6, \"removeNAs\" : False, \"removeRowLabels\" : True, \"numericallyEncode\" : True}\n",
        "  print(\"CNN predicting target location from Massa et al. (2024) dataset\")\n",
        "\n",
        "#details for second section of results - CNN predicting target location from Grubb & Li (2018) dataset\n",
        "elif replicateAnalysis == \"grubbLiTarget\":\n",
        "  details = {\"desiredLabel\": \"targetLocation\", \"inputFeatures\" : 2, \"outputClasses\" : 6, \"removeNAs\" : False, \"removeRowLabels\" : True, \"numericallyEncode\" : True}\n",
        "  print(\"CNN predicting target location from Grubb & Li (2018) dataset\")\n",
        "\n",
        "#details for third section of results - CNN predicting distractor location from Massa et al. (2024) dataset\n",
        "elif replicateAnalysis == \"massaDistractor\":\n",
        "  details = {\"desiredLabel\": \"valueDistractorLocation\", \"inputFeatures\" : 2, \"outputClasses\" : 6, \"removeNAs\" : True, \"removeRowLabels\" : True, \"numericallyEncode\" : True}\n",
        "  print(\"CNN predicting distractor location from Massa et al. (2024) dataset\")\n",
        "\n",
        "#details for fourth section of results - CNN predicting distractor location from Grubb & Li (2018) dataset\n",
        "elif replicateAnalysis == \"grubbLiDistractor\":\n",
        "  details = {\"desiredLabel\": \"distractorLocation\", \"inputFeatures\" : 2, \"outputClasses\" : 6, \"removeNAs\" : True, \"removeRowLabels\" : True, \"numericallyEncode\" : True}\n",
        "  print(\"CNN predicting distractor location from Grubb & Li (2018) dataset\")"
      ],
      "metadata": {
        "id": "2_ALLXvi3chp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in data"
      ],
      "metadata": {
        "id": "dmAwA_TSQ2XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upload = files.upload() #The GUI is actually much faster at this\n",
        "\n",
        "#load in data if replicating analyses using Massa et al. (2024) dataset\n",
        "if replicateAnalysis == \"massaTarget\" or replicateAnalysis == \"massaDistractor\":\n",
        "  feature1 = pd.read_csv('CMBG_BRM_Section4_XPos.csv')\n",
        "  feature2 = pd.read_csv('CMBG_BRM_Section4_YPos.csv')\n",
        "  conditions = pd.read_csv('CMBG_BRM_Section4_Conditions.csv')\n",
        "  print(\"Massa dataset read\")\n",
        "\n",
        "#load in data if replicating analyses using Grubb & Li (2018) dataset\n",
        "elif replicateAnalysis == \"grubbLiTarget\" or replicateAnalysis == \"grubbLiDistractor\":\n",
        "  feature1 = pd.read_csv('CMBG_BRM_Section2_XPos.csv')\n",
        "  feature2 = pd.read_csv('CMBG_BRM_Section2_YPos.csv')\n",
        "  conditions = pd.read_csv('CMBG_BRM_Section2_Conditions.csv')\n",
        "  print(\"Grubb & Li dataset read\")\n",
        "\n",
        "\n",
        "#format data into 3d numpy array\n",
        "if (details[\"inputFeatures\"] == 1):\n",
        "  arr2d = feature1.values\n",
        "  data = arr2d[:, np.newaxis,:]\n",
        "else:\n",
        "  data = np.stack((feature1, feature2), axis=1)\n",
        "#remove row labels if needed\n",
        "if details[\"removeRowLabels\"] == True:\n",
        "  data = data[:,:,1:]\n",
        "\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "RNOrAMS-SglC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract desired labels from conditions datafile"
      ],
      "metadata": {
        "id": "A1YKRS0cv8oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = details['desiredLabel']\n",
        "\n",
        "#remove NA values if necessary (e.g., if search object is absent on some trials)\n",
        "if details[\"removeNAs\"] == True:\n",
        "  data = data[conditions[label].isna()==False, :, :]\n",
        "  conditions = conditions[conditions[label].isna()==False]\n",
        "\n",
        "#extract label information from conditions file - this will serve as the comparison during supervised learning\n",
        "label = details['desiredLabel']\n",
        "labels = conditions[label].astype(int)\n",
        "\n",
        "#numerical encoding of labels -- The actual method of encoding can change based on your data.\n",
        "#the example encoding is what we used for our reported analyses.\n",
        "if details[\"numericallyEncode\"] == True:\n",
        "  labels = labels.map({0:0, 60:1, 120:2, 180:3, 240:4, 300:5})"
      ],
      "metadata": {
        "id": "7H3FVtj4v3U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels.head())"
      ],
      "metadata": {
        "id": "NRZucDRQUgY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training-validation split (66% training, 33% validation)"
      ],
      "metadata": {
        "id": "c-NfFNwuxivI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N, R, C = data.shape\n",
        "\n",
        "#find number of training/test trials (if data is not divisible by 3, add extras to training set)\n",
        "n_test = N // 3 #same as floor(N/3)\n",
        "n_train = N - n_test\n",
        "\n",
        "#predefine dataframes with corresponding trial lengths\n",
        "training_data = np.zeros((n_train, R, C))\n",
        "test_data = np.zeros((n_test, R, C))\n",
        "\n",
        "training_labels = []\n",
        "test_labels = []\n",
        "\n",
        "trainCounter = 0\n",
        "testCounter = 0\n",
        "\n",
        "for i in range(0, len(data)):\n",
        "  if i % 3 == 0: #every third trial, add data and label to validation set\n",
        "    test_data[testCounter, :, : ] = data[i, :, :]\n",
        "    test_labels.append(labels.iloc[i])\n",
        "    testCounter += 1\n",
        "  else: #otherwise, add data and label to training set\n",
        "    training_data[trainCounter, :, : ] = data[i, :, :]\n",
        "    training_labels.append(labels.iloc[i])\n",
        "    trainCounter += 1\n",
        "\n",
        "training_labels = pd.Series(training_labels)\n",
        "test_labels = pd.Series(test_labels)\n",
        "\n",
        "print(training_data.shape, test_data.shape)"
      ],
      "metadata": {
        "id": "mmk-AIP3xqOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process data"
      ],
      "metadata": {
        "id": "m9mb9MoATOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform numpy data to tensor\n",
        "training_data = torch.from_numpy(training_data)\n",
        "test_data = torch.from_numpy(test_data)\n",
        "\n",
        "#change to float type - more easily handled by training + test loops\n",
        "training_data = training_data.float()\n",
        "test_data = test_data.float()"
      ],
      "metadata": {
        "id": "pGX7ZkgOUhC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check dimensions of test data and labels"
      ],
      "metadata": {
        "id": "edIcRkTowJDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data.shape)\n",
        "#trials, features, samples\n",
        "\n",
        "print(test_labels.shape)\n",
        "# 1-dimensional tensor of identical length to rows of test data"
      ],
      "metadata": {
        "id": "BBQ8GhxzwO3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine size of flattened output passed to first linear transform -- different based on the length of your trial and number of features"
      ],
      "metadata": {
        "id": "Ph5-Bt0mEJx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sizeChecker = nn.Sequential( #the same layers as the first portion of the CNN, just to see what that layer's output size will be\n",
        "        nn.Conv1d(in_channels = details[\"inputFeatures\"], out_channels = 64, kernel_size = 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.MaxPool1d(kernel_size=5),\n",
        "        nn.Flatten()\n",
        "        )\n",
        "tmp = sizeChecker(test_data[0:1,:,:])\n",
        "flattenOutput = tmp.shape[1]\n",
        "print(flattenOutput)"
      ],
      "metadata": {
        "id": "f-1TWELREcce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the neural network\n"
      ],
      "metadata": {
        "id": "yN8rLWuNkoCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #Flattening layer (gets called after Sequential convolution layer)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        #Sequential layer containing convolution and subsequent regularization methods\n",
        "        self.cnnL1 = nn.Sequential(\n",
        "        nn.Conv1d(in_channels = details[\"inputFeatures\"], out_channels = 64, kernel_size = 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.25),\n",
        "        nn.MaxPool1d(kernel_size=5),\n",
        "        )\n",
        "\n",
        "        #Linear transforms\n",
        "        self.lin = nn.Linear(in_features = flattenOutput, out_features = 64)\n",
        "        self.lin2 = nn.Linear(in_features = 64, out_features = 32)\n",
        "        self.lin3 = nn.Linear(in_features = 32, out_features = details[\"outputClasses\"]) #Output is X number of logits, to be used as the CNN's predictions with loss function\n",
        "\n",
        "    def forward(self, x):\n",
        "        #apply convolution\n",
        "        conv = self.cnnL1(x)\n",
        "\n",
        "        #rearrage resulting array for proper order during flattening\n",
        "        permuted = conv.permute(0,2,1)\n",
        "\n",
        "        #flatten array into one-dimensional tensor\n",
        "        flattened = self.flatten(permuted)\n",
        "\n",
        "        #linear transforms, with logits as output\n",
        "        linear = self.lin(flattened)\n",
        "        linear2 = self.lin2(linear)\n",
        "        logits = self.lin3(linear2)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "ACZkKG4W8ljE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Early Stopping class"
      ],
      "metadata": {
        "id": "VMZ14NwJn5a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EarlyStopping class used in pytorchtools library\n",
        "#source: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pth', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "h4-kQmcMn5jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a Custom Dataset object that stores the samples and the labels together for each trial"
      ],
      "metadata": {
        "id": "8QBhH8cutUmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, labelsObject, dataObject, transform=None, target_transform=None):\n",
        "        self.labels = labelsObject\n",
        "        self.dataframe = dataObject\n",
        "        self.transform = transform #in case data needs to be transformed to a different type\n",
        "        self.target_transform = target_transform #in case labels need to be transformed to a different type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.dataframe[idx]\n",
        "        label = self.labels.iloc[idx]\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "xJXrHXi_D1Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the CustomDatasets for training and validation data"
      ],
      "metadata": {
        "id": "3XSb2czvtzbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = CustomDataset(labelsObject = training_labels, dataObject = training_data)\n",
        "test = CustomDataset(labelsObject = test_labels, dataObject = test_data)"
      ],
      "metadata": {
        "id": "xDWngfZatzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create DataLoader objects containing the training and test data/labels"
      ],
      "metadata": {
        "id": "gMMqp0N4UzoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train, batch_size=batch_size, drop_last=False)\n",
        "test_dataloader = DataLoader(test, batch_size=batch_size, drop_last=False)"
      ],
      "metadata": {
        "id": "lnJkTN0HFjEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiT4f2wY8M5a"
      },
      "source": [
        "## Hyperparameters & Optimizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxegQ-898M5a"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "epochs = 2500 #arbitrary large number to give time for early stopping to occur\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJGLYx-r8M5b"
      },
      "source": [
        "\n",
        "## Full Implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eshSqRXG8M5c"
      },
      "outputs": [],
      "source": [
        "def train_model(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train() #set the model to training mode - important for batch normalization and dropout layers\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        #compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        #backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print current loss every 20 batches\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_model(dataloader, model, loss_fn, trialLevelDF, epNum = 0, earlyStop=\"none\", lossChecker = \"none\", mode = \"loss\"):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval() #set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    #evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    with torch.no_grad():\n",
        "      batchCount = 0\n",
        "      for X, y in dataloader:\n",
        "          pred = model(X) #apply model to batch\n",
        "          predSize = len(pred)\n",
        "          for p in (range(predSize)): #iterate through current batch to get trial-level accuracies\n",
        "\n",
        "              #determine whether there was a correct or incorrect prediction on the current dataframe\n",
        "              trialAcc = (pred[p].argmax().item() == y[p].item())\n",
        "\n",
        "              #save trial-level accuracy to initialized dataframe\n",
        "              trialLevelDF.iloc[(batch_size*batchCount)+p,epNum] = trialAcc\n",
        "\n",
        "          #update loss and accuracy metrics for reporting\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item() #same calculation as above, but performed for entire batch at once\n",
        "          batchCount+=1\n",
        "\n",
        "    #compute overall loss and accuracy\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "\n",
        "    #if we want to save the CNN's lowest loss value, record that minimum loss and the epoch where it occurred\n",
        "    if lossChecker != \"none\" and test_loss < lossChecker[0]:\n",
        "      lossChecker[0] = test_loss\n",
        "      lossChecker[1] = epNum\n",
        "\n",
        "    #if we want to implement early stopping, apply early stopping\n",
        "    if earlyStop != \"none\":\n",
        "      earlyStop(test_loss, model)\n",
        "\n",
        "    #print the relevant performance metrics -- loss, accuracy, or both\n",
        "    if mode == \"loss\":\n",
        "      print(f\"Epoch {epNum+1} complete! \\n Current loss: {test_loss:>8f}    Lowest loss: {lossChecker[0]:>8f} \\n\")\n",
        "    elif mode == \"accuracy\":\n",
        "      print(f\"Accuracy: {(100*correct):>8f}% \\n\")\n",
        "    elif mode ==\"both\":\n",
        "      print(f\"Epoch {epNum+1} complete! \\n Current loss: {test_loss:>8f}    Lowest loss: {lossChecker[0]:>8f} \\n Accuracy: {(100*correct):>8f}% \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mScO9OrS8M5c"
      },
      "source": [
        "# Implement training and test procedures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz5_Cozo8M5c"
      },
      "outputs": [],
      "source": [
        "#for storing CNN validation accuracy in each epoch\n",
        "performanceSummary = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(epochs))\n",
        "\n",
        "#early stopping object\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "\n",
        "#for storing minimum loss value and epoch where it occurred\n",
        "lossCheck = [np.inf, 0]\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    #training\n",
        "    train_model(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "    #validation\n",
        "    test_model(test_dataloader, model, loss_fn, performanceSummary,\n",
        "               epNum = t, earlyStop = early_stopping, lossChecker=lossCheck, mode = \"loss\")\n",
        "\n",
        "    #check early stopping criteria\n",
        "    if early_stopping.early_stop:\n",
        "      print(f\"Early stopping @ epoch {t+1}\")\n",
        "      break\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick summary of model results"
      ],
      "metadata": {
        "id": "GMhTBfUHjs1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "bestEpoch = lossCheck[1]\n",
        "performanceSummary = performanceSummary.iloc[:,0:t+1] #in case early stopping happens\n",
        "epAcc = (100*(performanceSummary.sum() / len(test_dataloader.dataset)))\n",
        "bestPerformance = epAcc[bestEpoch]\n",
        "\n",
        "#plot accuracy across epochs and label accuracy from interation with lowest validation loss\n",
        "plt.plot(range(t+1), epAcc)\n",
        "plt.arrow(x = bestEpoch, y = bestPerformance - 0.1*bestPerformance, dx = 0, dy = 0.1*bestPerformance, width = .2)\n",
        "plt.text(x = bestEpoch-0.33*bestEpoch, y = bestPerformance - 0.2*bestPerformance, s = f\"Accuracy from epoch {bestEpoch} (starting \\n at 0) with the lowest loss -- \\n we use this in analysis!\")\n",
        "plt.text(x = bestEpoch-0.33*bestEpoch, y = bestPerformance - 0.3*bestPerformance, s = \"Note: Overfitting may have \\n happened in later epochs, \\n causing a severe accuracy \\n decrease\")\n",
        "\n",
        "#save graph of performance across epochs\n",
        "plt.savefig(\".png\")\n",
        "files.download(\".png\")\n",
        "\n",
        "#view graph and accuracy values across epochs\n",
        "plt.show()\n",
        "print(epAcc)"
      ],
      "metadata": {
        "id": "9TFvZMZk1p5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# View accuracy from epoch with lowest loss"
      ],
      "metadata": {
        "id": "dWDBEsa9Jcov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load in best weights\n",
        "bestWeights = torch.load(\"checkpoint.pth\")\n",
        "model.load_state_dict(bestWeights)\n",
        "\n",
        "#check of prediction accuracy for model\n",
        "trialLevelAcc = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(1))\n",
        "test_model(test_dataloader, model, loss_fn, trialLevelAcc, mode = \"accuracy\")"
      ],
      "metadata": {
        "id": "AKMDIOaePgmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download trial level accuracy as .csv file through Google Drive"
      ],
      "metadata": {
        "id": "pv_4aFEBFmPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelAcc.to_csv('.csv')\n",
        "files.download('.csv')"
      ],
      "metadata": {
        "id": "x9yZ_J6Rmcpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model parameter weights locally and download to computer"
      ],
      "metadata": {
        "id": "7_Yr1zQZrWeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \".pth\") #.pth file is type used for storing parameter weights\n",
        "files.download(\".pth\")"
      ],
      "metadata": {
        "id": "dYILF6j2We5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#View exact accuracy of the model from the corresponding results section\n",
        "If you would like to see the performance of the model that produced the classifcation accuracy reported in the corresponding results section, run the code below. **Prior to running this code,** upload the corresponding `.pth` file to the Colab environment."
      ],
      "metadata": {
        "id": "fyVaKQACB8a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to upload weights from previous model\n",
        "if replicateAnalysis == \"massaTarget\"\n",
        "  weightsName = \"model_state_MassaTargLoc.pth\"\n",
        "\n",
        "elif replicateAnalsis == \"grubbLiTarget\"\n",
        "  weightsName = \"model_state_GrubbLiTargLoc.pth\n",
        "\n",
        "elif replicateAnalysis == \"massaDistractor\"\n",
        "  weightsName = \"model_state_MassaDistLoc.pth\"\n",
        "\n",
        "elif replicateAnalysis == \"grubbLiDistractor\"\n",
        "  weightsName = \"model_state_GrubbLiDistLoc.pth\"\n",
        "\n",
        "#load in weights\n",
        "modelWeights = torch.load(weightsName)\n",
        "model.load_state_dict(modelWeights)\n",
        "\n",
        "#check accuracy\n",
        "tmp = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(1))\n",
        "test_model(test_dataloader, model, loss_fn, tmp, mode = \"accuracy\")"
      ],
      "metadata": {
        "id": "WzdzbEXAa9WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculate SHAP values for each trial of validation set\n",
        "Fair warning, this takes a substantial amount of time to run."
      ],
      "metadata": {
        "id": "JAjSseyvBIWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelSHAP = pd.DataFrame(index=range(len(test_dataloader.dataset)), columns = range(len(test_dataloader.dataset[0][0][0])))\n",
        "print(trialLevelSHAP.shape)\n",
        "\n",
        "shapBatch = test_dataloader.dataset[0:100] #first 100 trials used for explanation model\n",
        "explainer = shap.DeepExplainer(model, shapBatch[0])\n",
        "\n",
        "loopDur = len(test_dataloader.dataset)\n",
        "for s in range(loopDur):\n",
        "\n",
        "  #calculate SHAP values relative to explainer set\n",
        "  shap_values = explainer.shap_values(test_dataloader.dataset[s:s+1][0])\n",
        "\n",
        "  #compute global feature importance by taking absolute value\n",
        "  abs_shap_values = np.abs(shap_values)\n",
        "\n",
        "  #average across classes and features to get one global feature importance value per sample\n",
        "  SHAP = abs_shap_values.mean(axis=(0,1,3))\n",
        "\n",
        "  #save trial-level accuracy to initialized dataframe\n",
        "  trialLevelSHAP.iloc[s,:] = SHAP\n",
        "  if s % 100 ==0:\n",
        "    print(s)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "WEkHk05hDwAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download SHAP values as .csv file"
      ],
      "metadata": {
        "id": "LK2cKbmYJiji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trialLevelSHAP = pd.DataFrame(trialLevelSHAP)\n",
        "trialLevelSHAP.to_csv('.csv')\n",
        "files.download('.csv')"
      ],
      "metadata": {
        "id": "Hu5KpAP6jBcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Graphing heatmap of results for last trial as a quick check\n",
        "df = pd.DataFrame({\"SHAP values for each sample in timecourse\": abs_shap_values.mean(axis=(0,1,3))},\n",
        "                  index=range(len(shap_values[0][0])))\n",
        "g = sns.heatmap(df, fmt=\"g\", cmap='viridis')\n",
        "g.set_yticks(range(0,600,50))\n",
        "g.set_yticklabels(range(0,600,50))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YbdUOf8I3VXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}